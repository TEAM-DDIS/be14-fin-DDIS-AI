# app.py

import os
import json
import asyncio
from dotenv import load_dotenv

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from datetime import datetime

# Embedding & Vector store
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import time

# Chains & Memory (ìµœì‹  ë°©ì‹)
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage
from langchain.memory import ConversationBufferMemory

from langchain_core.runnables import RunnableLambda
from langchain_core.messages import SystemMessage
import httpx

load_dotenv()

app = FastAPI()

# CORS (í”„ë¡ íŠ¸ì—”ë“œ ì£¼ì†Œì— ë§ê²Œ ì¡°ì •)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡ íŠ¸ ì£¼ì†Œ
    allow_methods=["*"],
    allow_headers=["*"],
)

# â€”â€”â€” ë°ì´í„° ëª¨ë¸ â€”â€”â€”
class QueryPayload(BaseModel):
    question: str

# â€”â€”â€” Retriever ì •ì˜ â€”â€”â€”
def get_retriever(
    persist_dir="chroma_local_policies",
    model_name="text-embedding-ada-002",
    k=3,
    fetch_k=10,
    lambda_mult=0.85,
):
    embeddings = OpenAIEmbeddings(
        model=model_name,
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )
    chroma = Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings,
    )
    return chroma.as_retriever(
        search_type="mmr",
        search_kwargs={"fetch_k": fetch_k, "k": k, "lambda_mult": lambda_mult},
    )

retriever = get_retriever()

# â€”â€”â€” LLM & Memory ì •ì˜ â€”â€”â€”
OPENAI_KEY = os.getenv("OPENAI_API_KEY") or ""
llm = ChatOpenAI(
    temperature=0.0,
    model="gpt-4o-mini",
    openai_api_key=OPENAI_KEY,
)
shared_memory = ConversationBufferMemory(return_messages=True)

# RAG ì²´ì¸ (ë¹„ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
RAG_PROMPT = PromptTemplate(
    input_variables=[
        "currentDate",           
        "context", "question",
        "employeeName", "departmentName", "teamName",
        "rankName", "positionName", "jobName",
        "hireDate", "remainingVacationDays",
        "draftsCount", "rejectedCount", "teamMembers", "personalSchedules"
    ],
    template=(
        "ë‹¹ì‹ ì€ íšŒì‚¬ì˜ ê³µì‹ ë¹„ì„œì…ë‹ˆë‹¤.\n"
        "ğŸ—“ï¸ í˜„ì¬ ë‚ ì§œ: {currentDate}\n"
        "ì•„ë˜ **ë¬¸ì„œ**ì™€ **ì‚¬ìš©ì ì •ë³´**ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì ˆëŒ€ ë¬¸ì„œ ì™¸ì˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n\n"
        "ì ˆëŒ€ ì•Œê³  ìˆëŠ” ì‚¬ìš©ì ì •ë³´ì™¸ì— ì¶”ê°€í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n\n"

        "=== ì‚¬ìš©ì ì •ë³´ ===\n"
        "- ì´ë¦„: {employeeName}\n"
        "- ë¶€ì„œ: {departmentName}\n"
        "- íŒ€: {teamName}\n"
        "- ì§ê¸‰: {rankName}\n"
        "- ì§ì±…: {positionName}\n"
        "- ì§ë¬´: {jobName}\n"
        "- ì…ì‚¬ì¼: {hireDate}\n"
        "- ì”ì—¬ íœ´ê°€ì¼: {remainingVacationDays}ì¼\n"
        "- ê¸°ì•ˆ ë¬¸ì„œ: {draftsCount}ê±´\n"
        "- ê²°ì¬ ëŒ€ê¸° ë¬¸ì„œ: {pendingCount}ê±´\n"
        "- ë°˜ë ¤ëœ ë¬¸ì„œ: {rejectedCount}ê±´\n"
        "- íŒ€ì›: {teamMembers}\n\n"

        "=== ê°œì¸ ì¼ì • ===\n"
        "{personalSchedules}\n\n"

        "=== ì‚¬ë‚´ ê·œì • ë¬¸ì„œ ===\n"
        "{context}\n\n"

        "â“ **ì§ˆë¬¸:** {question}\n\n"

        "ğŸ’¬ **ë‹µë³€ ì‘ì„± ê·œì¹™**\n"
        "1. ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì •í™•íˆ ì¸ìš©í•˜ë˜, í˜ì´ì§€ë‚˜ ì„¹ì…˜ ë²ˆí˜¸ë¥¼ ê´„í˜¸ ì•ˆì— í‘œì‹œí•˜ì„¸ìš”.\n"
        "2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” â€œë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.â€ë¼ê³ ë§Œ ë‹µí•˜ì‹­ì‹œì˜¤.\n"
        "3. ë¬¸ì¥ì€ 2~3ê°œ ë‹¨ë½ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ì¤‘ìš”í•œ í‚¤ì›Œë“œëŠ” êµµê²Œ í‘œì‹œí•˜ì„¸ìš”.\n\n"

        "**ë‹µë³€:**"
    )
)

# â€”â€”â€” ì²´ì¸ ì¡°ë¦½ â€”â€”â€”
def format_docs(docs):
    return "\n\n".join([doc.page_content for doc in docs])

# Chat ì²´ì¸ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
def chat_chain(input_text: str) -> str:
    history = shared_memory.load_memory_variables({})["history"]
    history.append(HumanMessage(content=input_text))
    response = llm.invoke(history)
    shared_memory.save_context({"input": input_text}, {"output": response.content})
    return response.content

def build_rag_chain(user_info: dict, llm_instance, current_date: str):
    # ì‚¬ìš©ì ì •ë³´ ëŒë‹¤ ëª¨ìŒ
    user_vars = {
        "currentDate": RunnableLambda(lambda _: current_date),
        "employeeName": RunnableLambda(lambda _: user_info.get("employeeName","ì •ë³´ ì—†ìŒ")),
        "departmentName": RunnableLambda(lambda _: user_info.get("departmentName","ì •ë³´ ì—†ìŒ")),
        "teamName": RunnableLambda(lambda _: user_info.get("teamName","ì •ë³´ ì—†ìŒ")),
        "rankName": RunnableLambda(lambda _: user_info.get("rankName","ì •ë³´ ì—†ìŒ")),
        "positionName": RunnableLambda(lambda _: user_info.get("positionName","ì •ë³´ ì—†ìŒ")),
        "jobName": RunnableLambda(lambda _: user_info.get("jobName","ì •ë³´ ì—†ìŒ")),
        "hireDate": RunnableLambda(lambda _: user_info.get("hireDate","ì •ë³´ ì—†ìŒ")),
        "remainingVacationDays": RunnableLambda(lambda _: str(user_info.get("remainingVacationDays","ì •ë³´ ì—†ìŒ"))),
        "draftsCount": RunnableLambda(lambda _: str(user_info.get("draftsCount",0))),
        "pendingCount": RunnableLambda(lambda _: str(user_info.get("pendingCount",0))),
        "rejectedCount": RunnableLambda(lambda _: str(user_info.get("rejectedCount",0))),
        "teamMembers": RunnableLambda(lambda _: ", ".join(
            m["employeeName"] for m in user_info.get("teamMembers",[]) if m.get("employeeName")
        )),
        "personalSchedules": RunnableLambda(lambda _: "\n".join(
            f"- {s['scheduleDate']} {s['scheduleTime']} : {s['scheduleTitle']}"
            for s in user_info.get("personalSchedules",[])
        )),
    }

    spec = {"context": retriever | format_docs, "question": RunnablePassthrough()}
    spec.update(user_vars)
    return (spec | RAG_PROMPT) | llm_instance

# â”€â”€â”€ USER_INFO ì „ìš© ì œë„ˆë ˆì´í„° â”€â”€â”€
async def user_info_generator(user_info: dict, question: str):
    # 1. ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— í”„ë¡œí•„ ë‹´ê¸°
    profile = format_user_profile(user_info)
    system_msg = SystemMessage(content=f"ğŸ‘¤ ì‚¬ìš©ì ì •ë³´:\n{profile}")
    user_msg = HumanMessage(content=question)
    today   = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    date_msg = SystemMessage(content=f"ğŸ—“ï¸ í˜„ì¬ ë‚ ì§œ: {today}")

    # 2. ìŠ¤íŠ¸ë¦¬ë° LLM ì„¸íŒ…
    llm_stream = ChatOpenAI(
        temperature=0.0,
        model="gpt-4o-mini",
        streaming=True,
        openai_api_key=OPENAI_KEY,
    )

    # 3. ë©”ì‹œì§€ ìˆœì„œ: [System, User]
    async for chunk in llm_stream.astream([system_msg, user_msg,date_msg]):
        yield chunk.content


_user_info_cache: dict[str, tuple[dict, float]] = {}
TTL = 300  # 5ë¶„
async def fetch_user_info(token: str) -> dict:
    entry = _user_info_cache.get(token)
    if entry and time.time() - entry[1] < TTL:
        return entry[0]
    async with httpx.AsyncClient() as client:
        response = await client.get(
            "http://localhost:5000/chatbot",
            headers={"Authorization": token}
        )

        print(f"ğŸ” Spring ì‘ë‹µ ì½”ë“œ: {response.status_code}")
        print(f"ğŸ” Spring ì‘ë‹µ ë‚´ìš©: {response.text}")

        response.raise_for_status()
        data = response.json()
        _user_info_cache[token] = (data, time.time())
        return data
    

def format_user_profile(user_info: dict) -> str:
    lines = [
        f"ì´ë¦„: {user_info.get('employeeName', 'ì •ë³´ ì—†ìŒ')}",
        f"ë¶€ì„œ: {user_info.get('departmentName', 'ì •ë³´ ì—†ìŒ')}",
        f"íŒ€: {user_info.get('teamName', 'ì •ë³´ ì—†ìŒ')}",
        f"ì§ê¸‰: {user_info.get('rankName', 'ì •ë³´ ì—†ìŒ')}",
        f"ì§ì±…: {user_info.get('positionName', 'ì •ë³´ ì—†ìŒ')}",
        f"ì§ë¬´: {user_info.get('jobName', 'ì •ë³´ ì—†ìŒ')}",
        f"ì…ì‚¬ì¼: {user_info.get('hireDate', 'ì •ë³´ ì—†ìŒ')}",
        f"ë‚¨ì€ ì—°ì°¨: {user_info.get('remainingVacationDays', 'ì •ë³´ ì—†ìŒ')}ì¼",
        f"ê¸°ì•ˆí•œ ë¬¸ì„œ ìˆ˜: {user_info.get('draftsCount', 0)}ê±´",
        f"ê²°ì¬ ëŒ€ê¸° ë¬¸ì„œ ìˆ˜: {user_info.get('pendingCount', 0)}ê±´",
        f"ë°˜ë ¤ ë¬¸ì„œ ìˆ˜: {user_info.get('rejectedCount', 0)}ê±´",
    ]

    team = user_info.get("teamMembers", [])
    if team:
        team_names = [m["employeeName"] for m in team if m.get("employeeName")]
        lines.append(f"ê°™ì€ íŒ€ íŒ€ì›: {', '.join(team_names)}")

    schedules = user_info.get("personalSchedules", [])
    if schedules:
        formatted_schedules = "\n".join([
            f"- {s['scheduleDate']} {s['scheduleTime']} : {s['scheduleTitle']}" for s in schedules
        ])
        lines.append(f"\nğŸ“† ê°œì¸ ì¼ì •:\n{formatted_schedules}")

    return "\n".join(lines)

router_prompt = PromptTemplate(
    input_variables=["question"],
    template=(
        "ğŸ” **ì‚¬ë‚´ ê·œì •Â·ì •ì±…Â·ì ˆì°¨ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ë°˜ë“œì‹œ CHATìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.**\n"
        "â—ï¸ì •ì±…Â·ê·œì •Â·í”„ë¡œì„¸ìŠ¤Â·íœ´ê°€Â·ê·¼íƒœÂ·ì¸ì‚¬ë°œë ¹Â·ë³µì§€ ë“± íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ RETRIEVEë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n"
        "ğŸ‘¤ ê°œì¸ì •ë³´ ì¡°íšŒ(ë‚´ ì—°ì°¨, ë‚´ ê¸°ì•ˆ ìƒíƒœ ë“±)ëŠ” USER_INFOë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n"
        "ğŸ’¬ ê·¸ ì™¸ ì¼ë°˜ ëŒ€í™”, ë¸Œë ˆì¸ìŠ¤í† ë°, ì¡ë‹´ ë“±ì€ CHATìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n\n"
        "â”€â”€ **ì˜ˆì‹œ: RETRIEVE** â”€â”€\n"
        "Q: ìš°ë¦¬ íšŒì‚¬ ì—°ì°¨ ê·œì •ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n"
        "A: RETRIEVE\n"
        "Q: íœ´ê°€ ì¢…ë¥˜ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n"
        "A: RETRIEVE\n"
        "Q: ì¸ì‚¬ë°œë ¹ ì ˆì°¨ ì•Œë ¤ì¤˜\n"
        "A: RETRIEVE\n\n"
        "â”€â”€ **ì˜ˆì‹œ: USER_INFO** â”€â”€\n"
        "Q: ë‚´ ë‚¨ì€ ì—°ì°¨ê°€ ì–¼ë§ˆë‚˜ ìˆë‚˜ìš”?\n"
        "A: USER_INFO\n"
        "Q: ì œê°€ ì˜¬ë¦° ê¸°ì•ˆ ëª‡ ê±´ ìˆì£ ?\n"
        "A: USER_INFO\n\n"
        "â”€â”€ **ì˜ˆì‹œ: CHAT** â”€â”€\n"
        "Q: ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\n"
        "A: CHAT\n"
        "Q: ì ì‹¬ ë­ ë¨¹ì„ì§€ ì¶”ì²œí•´ì¤˜\n"
        "A: CHAT\n"
        "Q: ì£¼ë§ì— ë­ í• ê¹Œ?\n"
        "A: CHAT\n\n"
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µ:"
    )
)
router_chain = router_prompt | llm | (lambda x: x.content.strip().upper())

# â€”â€”â€” ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ â€”â€”â€”
@app.post("/query-stream")
async def query_stream(payload: QueryPayload, request: Request):
    token = request.headers.get("Authorization")  # âœ… í”„ë¡ íŠ¸ì—ì„œ ì˜¨ í† í° ë°›ê¸°
    user_info = await fetch_user_info(token) 
    question = payload.question
    route = router_chain.invoke({"question": question})

    # ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
    employee_id = user_info.get("employeeId") 
    if not employee_id:
        return {"error": "employeeIdê°€ ì—†ìŠµë‹ˆë‹¤"}

    user_profile = format_user_profile(user_info)


    # 2) ìŠ¤íŠ¸ë¦¬ë° LLM ì¤€ë¹„
    llm_stream = ChatOpenAI(
        temperature=0.0,
        model="gpt-4o-mini",
        streaming=True,
        openai_api_key=OPENAI_KEY,
    )

    today   = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

    if route == "USER_INFO":
    
        # ê°œì¸ í”„ë¡œí•„ ì§ˆë¬¸: ë¬¸ì„œ ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ ë‹µë³€
        return StreamingResponse(
            user_info_generator(user_info, question),
            media_type="text/plain"
        )


    elif route == "RETRIEVE":
        # RAG ì²´ì¸(streaming) â€” build_rag_chain ë‚´ë¶€ì—ì„œ retrieverë„ ëŒì•„ê°‘ë‹ˆë‹¤.
        rag_chain = build_rag_chain(user_info, llm_stream,today)

        async def rag_generator():
            full_resp = ""
            
            # âš ï¸ ì—¬ê¸´ dict í˜•íƒœë¡œ ë„˜ê²¨ì•¼ context(search) ë³€ìˆ˜ì— questionì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.
            async for chunk in rag_chain.astream(question):
                full_resp += chunk.content
                yield chunk.content
            shared_memory.save_context({"input": question}, {"output": full_resp})

        return StreamingResponse(rag_generator(), media_type="text/plain")

    else:  # CHAT ë¶„ê¸°
        async def chat_generator():
            # 1) íˆìŠ¤í† ë¦¬ + ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
            today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
            date_msg= SystemMessage(content=f"ğŸ—“ï¸ í˜„ì¬ ë‚ ì§œ: {today}")
            history = shared_memory.load_memory_variables({})["history"]
            profile = format_user_profile(user_info)
            system_msg = SystemMessage(content=f"ğŸ‘¤ ì‚¬ìš©ì ì •ë³´:\n{profile}")
            msgs = [date_msg,system_msg] + history + [HumanMessage(content=question)]

            # 2) ìŠ¤íŠ¸ë¦¬ë° + ë©”ëª¨ë¦¬ ì €ì¥
            full = ""
            async for chunk in llm_stream.astream(msgs):
                full += chunk.content
                yield chunk.content
            shared_memory.save_context({"input": question}, {"output": full})
 

        return StreamingResponse(chat_generator(), media_type="text/plain")

    
    

# â€”â€”â€” ë™ê¸°í˜• QA ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒ) â€”â€”â€”
class QueryResponse(BaseModel):
    answer: str
    sources: list[str]

@app.post("/qa", response_model=QueryResponse)
async def qa(payload: QueryPayload, request: Request):
    token = request.headers.get("Authorization")
    user_info = await fetch_user_info(token)
    question = payload.question
    route = router_chain.invoke({"question": question})

    if route == "RETRIEVE":
        today     = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
        # 2) build_rag_chain í˜¸ì¶œ (streaming ì•„ë‹Œ ë™ê¸°í˜•ì´ë‹ˆê¹Œ llm ì‚¬ìš©)
        rag_chain = build_rag_chain(user_info, llm, today)
        # 3) ì§ˆë¬¸ ë¬¸ìì—´ë§Œ ë„˜ê²¨ì„œ invoke
        response  = rag_chain.invoke(question)
        shared_memory.save_context({"input": question}, {"output": response.content})

        docs    = retriever.get_relevant_documents(question)
        sources = [d.metadata.get("source", "") for d in docs]
        return QueryResponse(answer=response.content, sources=sources)
    else:
        ans = chat_chain(question)
        return QueryResponse(answer=ans, sources=[])

# â€”â€”â€” í—¬ìŠ¤ì²´í¬ â€”â€”â€”
@app.get("/")
def root():
    return {"status": "ok"}
